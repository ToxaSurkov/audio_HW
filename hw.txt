Домашнее задание по распознаванию речи.

Цель — до обучить нейронную сеть распознавания речи  wav2vec2 base под русский язык. Показать , что вы добились улучшения качества распознавания , по сравнению с моделью из репозитария.

Дано:

1) Ноутбук до обучения   wav2vec2 - https://github.com/sovse/tutorial_wav2vec2/blob/master/Fine-Tune-Wav2vec.ipynb

2) Веса модели прошедшего 50% до обучения на данных  Common Voice русского языка https://disk.yandex.ru/d/Cpr7H0qFocWi6w

3) Токинезатор ( алфавит) модели https://github.com/sovse/tutorial_wav2vec2/blob/master/vocab.json



Задание:
Для получения 7 балов :
Вариант 1.
1) Сделать свой форк репозитария https://github.com/sovse/tutorial_wav2vec2
2) В нем исправить ноутбук https://github.com/sovse/tutorial_wav2vec2/blob/master/Fine-Tune-Wav2vec.ipynb, чтоб обучение стартовало с модели взятой из https://disk.yandex.ru/d/Cpr7H0qFocWi6w
с токинезатором https://github.com/sovse/tutorial_wav2vec2/blob/master/vocab.json

3) Поставить до обучаться свою модель. Убедится что качество модели улучшается.

4) Сохранить свою модель на яндекс или googl диски. Указать ссылку в своём форке на нее.

5) Сделать сравнение качества базовой модели (https://disk.yandex.ru/d/Cpr7H0qFocWi6w)и вашей. Посчитать wer  на произвольном тестовом датасете для двух моделей. Привести примеры вывода вашей модели и базовой.

Вариант 2.
1) Сделать свой форк репозитария https://github.com/sovse/tutorial_wav2vec2
1) Надо добавить исправления в ноутбук  https://github.com/sovse/tutorial_wav2vec2/blob/master/Fine-Tune-Wav2vec.ipynb, чтоб  можно было обучатся на данных , размером больше 1000 часов, то есть   предусмотреть , что весь датасет не поместится в оперативной памяти сервера обучения, а будет лежать в папке на диске. 
2) Добавить в программу обучения аугментация данных , перед подачей их в нейронную сеть.
3) Проверить , что ваш код обучения рабочий. Само обучения можно не производить.

Для получения 10 балов:
1) Подготовить датасет обучения размером больше 1000 часов аудио.
Данные можно взять  из :Сбер Голос (1240 часов https://www.openslr.org/114/)
или любых других открытых источников.  
2) Сделать свой форк репозитария https://github.com/sovse/tutorial_wav2vec2
3) Надо добавить исправления в свой форк  в ноутбуке  https://github.com/sovse/tutorial_wav2vec2/blob/master/Fine-Tune-Wav2vec.ipynb, чтоб  можно было обучатся на подготовленных вами данных, которые будут лежать в папке на локальном диске. 
Добавить в программу обучения аугментация данных , перед подачей их в нейронную сеть.
Начать до обучение с весов модели взятой из https://disk.yandex.ru/d/Cpr7H0qFocWi6w
с токинезатором https://github.com/sovse/tutorial_wav2vec2/blob/master/vocab.json

4) Поставить до обучаться свою модель на данных более 1000 часах аудио. Убедится что качество модели улучшается.

5) Сохранить свою модель на яндекс или googl диски. Указать ссылку в своем форке на нее.

6) Сделать сравнение качества базовой модели (https://disk.yandex.ru/d/Cpr7H0qFocWi6w)и вашей. Посчитать wer  на произвольном тестовом датасете для двух моделей. Привести примеры вывода вашей модели и базовой.

Для получения 12 балов:
1) Надо выполнить все задания для 10 балов.
2) Создать лингво модель на данных с https://disk.yandex.ru/d/nhehWn4uS7UKjA для улучшения качества распознавания.
3) До обучить модель до уровня не ниже wer=25 на тесте Сбер Голос Farfield.

